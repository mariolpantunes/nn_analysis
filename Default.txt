Dense NN standard model parameters: 
-----------------------------------

input shape -> 

hidden layer dim ->        

n_hidden_layers ->	1		<5 

https://www.researchgate.net/publication/338003075_Variations_in_the_Number_of_Layers_and_the_Number_of_Neurons_in_Artificial_Neural_Networks_Case_Study_of_Pattern_Recognition		
		
-			https://www.sciencedirect.com/science/article/pii/S240584402030952X

-	pag 158		https://books.google.pt/books?id=Swlcw7M4uD8C&printsec=frontcover&redir_esc=y#v=onepage&q&f=false


activaction function ->  relu - https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/ - https://www.v7labs.com/blog/neural-networks-activation-functions#h5 - https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c	-	https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6

task activation classificação -> 1 node: SIGMOID ; multiclass (1 nó por classificação): SOFTMAX p.e maçã verde, pera amarela, maçã amarela...; multilabel: SIGMOID p.e maçã/pêra verde/amarela   https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/    -    https://www.v7labs.com/blog/neural-networks-activation-functions#h5  -   https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c 

task activation regressão ->   linear	-	https://towardsdatascience.com/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8	-	https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c		-	https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6	-	https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6	

task nodes-> 1-regressão  https://www.enthought.com/blog/neural-network-output-layer	-	; 1 ou 2-classific binaria ; x-multiclass	

optimizer -> Adam   https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0 -  https://www.analyticsvidhya.com/blog/2021/09/a-comprehensive-guide-on-neural-networks-performance-optimization/  -   https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6 

loss -> Regressão: MSE -	https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/	-	https://towardsdatascience.com/most-common-loss-functions-in-machine-learning-c7212a99dae0	-		https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/

	Classificação: Binary Cross Entropy Loss  2 classes - https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/	- 	https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/  // Categorical Cross Entropy Loss Multi class -  	https://www.section.io/engineering-education/understanding-loss-functions-in-machine-learning/






CNN

input shape ->

n_kernels -> 16 ou 54 https://towardsdatascience.com/an-introduction-to-convolutional-neural-networks-eb0b60b58fd7 

kernel_size -> 3x3 ou 5x5	-	https://medium.com/analytics-vidhya/how-to-choose-the-size-of-the-convolution-filter-or-kernel-size-for-cnn-86a55a1e2d15	-	https://www.sicara.fr/blog-technique/2019-10-31-convolutional-layer	-convolution-kernel		https://medium.com/analytics-vidhya/significance-of-kernel-size-200d769aecb1		-		https://cs231n.github.io/convolutional-networks/

pool_size -> 2x2 		https://programmathically.com/what-is-pooling-in-a-convolutional-neural-network-cnn-pooling-layers-explained/	-	https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/

n_hidden_layers ->  1  <5 (mesmos links da dense)

activation_function ->  relu	-	https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6	-	https://www.v7labs.com/blog/neural-networks-activation-functions	-	https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c
				

task_activation -> classificação:	binary classification: sigmoid; 	multiclass softmax; multilabel = sigmoid	https://insightsimaging.springeropen.com/articles/10.1007/s13244-018-0639-9/tables/2
			regressão:	linear 													https://insightsimaging.springeropen.com/articles/10.1007/s13244-018-0639-9/tables/2



task_nodes -> ; 1 regressão ou 1 ou 2 -classific binaria ; x-multiclass	



GRU:

input shape ->

hidden layer dim -> 	 

n_hidden layers -> 	 1  <5 (mesmos links da dense)


activation function -> 	tanh > sigmoid 	-	https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c	-	https://www.v7labs.com/blog/neural-networks-activation-functions	-	https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/

task activation ->	tanh  (mesmos links da activation function)


task nodes ->	1	-	https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnnty	-	https://k21academy.com/datascience/machine-learning/recurrent-neural-networks/


loss -> classificação: binary cross entropy para label binaria -	categorical cross entropy -	mse	-	https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/	-	https://training.galaxyproject.org/archive/2021-06-01/topics/statistics/tutorials/RNN/tutorial.html	-	https://prvnk10.medium.com/loss-function-recurrent-neural-networks-rnns-ba4eb786d810
n falta o loss no keras regressor?
